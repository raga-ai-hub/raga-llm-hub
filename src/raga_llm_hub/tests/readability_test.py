"""
Readability Score
"""

import textstat


def extract_submetrics(text):
    """
    Provides a score for the readability.

    Args:
        prompt(str): the prompt given to the model
        response(str, optional): the response generated by the model
        threshold(float): The threshold for readability score (default is 5).

    Returns:
        dict: A dictionary containing the prompt, response, readability score, is_passed,
              submetrics for prompt and response, and threshold
    """
    return {
        "lexicon_count": textstat.lexicon_count(text),
        "syllable_count": textstat.syllable_count(text),
        "sentence_count": textstat.sentence_count(text),
        "character_count": textstat.char_count(text),
        "letter_count": textstat.letter_count(text),
        "polysyllable_count": textstat.polysyllabcount(text),
        "monosyllable_count": textstat.monosyllabcount(text),
        "difficult_words": textstat.difficult_words(text),
        "automated_readability_index": textstat.automated_readability_index(text),
    }


# this is using extra metrics for calculation: sentence_count, character_count, letter_count, polysylable_count,
# monosyllablecount, difficult_words, automated_readability_index
def readability_test(prompt, response=None, threshold=5):
    """
    Provides a score for the readability.

    Args:
        prompt(str): the prompt given to the model
        response(str, optional): the response generated by the model
        threshold(float): The threshold for readability score (default is 5).

    Returns:
        dict: A dictionary containing the prompt, response, readability score, is_passed,
              submetrics for prompt and response, and threshold
    """
    prompt_submetrics = extract_submetrics(prompt)
    if response is not None:
        response_submetrics = extract_submetrics(response)
    else:
        response_submetrics = None

    prompt_readability_average = sum(prompt_submetrics.values()) / len(
        prompt_submetrics
    )

    if response_submetrics is not None:
        response_readability_average = sum(response_submetrics.values()) / len(
            response_submetrics
        )
        is_readable = (
            True
            if (
                prompt_readability_average > threshold
                and response_readability_average > threshold
            )
            else False
        )
    else:
        response_readability_average = None
        is_readable = True if (prompt_readability_average <= threshold) else False

    reason = ""

    # Adding Prompt heading
    reason += "Prompt:\n"
    reason += "\n".join(f"{key}: {value}" for key, value in prompt_submetrics.items())
    reason += "\n"

    # Adding Response heading
    reason += "\nResponse:\n"
    reason += "\n".join(f"{key}: {value}" for key, value in response_submetrics.items())

    result = {
        "prompt": prompt,
        "response": response,
        "score": {
            "prompt_readability_score": prompt_readability_average,
            "response_readability_score": response_readability_average,
        },
        "is_passed": is_readable,
        "prompt_submetrics": prompt_submetrics,
        "response_submetrics": response_submetrics,
        "reason": reason,
        "threshold": threshold,
    }

    return result
